{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import tabulate\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/resnext/40X/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for l in os.listdir(data_path):\n",
    "    tempx = np.load(data_path+l)\n",
    "    tempy = (l[4]=='M')\n",
    "    x.append(tempx)\n",
    "    y.append(tempy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(zip(x, y))\n",
    "data = np.array(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(1995):\n",
    "    if(np.array_equal(x[i], data[i][0]) and y[i]==(data[i][1])):\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.nn.Sequential()\n",
    "# model = torch.nn.Linear(2048, 1024)\n",
    "# model = F.relu(model)\n",
    "# model = torch.nn.Linear(1024, 512)\n",
    "# model = F.relu(model)\n",
    "# model = torch.nn.Linear(512, 64)\n",
    "# model = F.relu(model)\n",
    "# model = torch.nn.Linear(64, 2)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1995, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:1500]\n",
    "train_x, train_y = [], []\n",
    "for i in train:\n",
    "    train_x.append(np.squeeze(i[0]))\n",
    "    train_y.append([float(i[1])])\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "val = data[1500:1800]\n",
    "val_x, val_y = [], []\n",
    "for i in val:\n",
    "    val_x.append(np.squeeze(i[0]))\n",
    "    val_y.append([float(i[1])])\n",
    "val_x = np.array(val_x)\n",
    "val_y = np.array(val_y)\n",
    "test = data[1800:]\n",
    "test_x, test_y = [], []\n",
    "for i in test:\n",
    "    test_x.append(np.squeeze(i[0]))\n",
    "    test_y.append([float(i[1])])\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 2048), (1500, 1), (300, 2048), (300, 1), (195, 2048), (195, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, val_x.shape, val_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x = torch.stack([torch.Tensor(i) for i in train_x]) # transform to torch tensors\n",
    "t_y = torch.stack([torch.Tensor(i) for i in train_y])\n",
    "my_dataset = utils.TensorDataset(t_x,t_y) # create your datset\n",
    "train_dataloader = utils.DataLoader(my_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_x = torch.stack([torch.Tensor(i) for i in val_x]) # transform to torch tensors\n",
    "v_y = torch.stack([torch.Tensor(i) for i in val_y])\n",
    "my_dataset = utils.TensorDataset(v_x,v_y) # create your datset\n",
    "val_dataloader = utils.DataLoader(my_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_x = torch.stack([torch.Tensor(i) for i in test_x]) # transform to torch tensors\n",
    "te_y = torch.stack([torch.Tensor(i) for i in test_y])\n",
    "my_dataset = utils.TensorDataset(te_x,te_y) # create your datset\n",
    "test_dataloader = utils.DataLoader(my_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1500, 2048]),\n",
       " torch.Size([1500, 1]),\n",
       " torch.Size([300, 2048]),\n",
       " torch.Size([300, 1]),\n",
       " torch.Size([195, 2048]),\n",
       " torch.Size([195, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x.shape, t_y.shape, v_x.shape, v_y.shape, te_x.shape, te_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praty\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  1.1462979247266816 Valid Loss:  0.658735742966334\n",
      "Epoch: 2 Training Loss:  0.6445913439691067 Valid Loss:  0.6447298125425974\n",
      "Epoch: 3 Training Loss:  0.631910047069192 Valid Loss:  0.6414164567987124\n",
      "Epoch: 4 Training Loss:  0.6233120981454849 Valid Loss:  0.6411884177724521\n",
      "Epoch: 5 Training Loss:  0.6221676084597906 Valid Loss:  0.6417685222625732\n",
      "Epoch: 6 Training Loss:  0.6210772732496261 Valid Loss:  0.6422578972578049\n",
      "Epoch: 7 Training Loss:  0.6283951601063211 Valid Loss:  0.6423569172620773\n",
      "Epoch: 8 Training Loss:  0.6209591676990192 Valid Loss:  0.6425964480638504\n",
      "Epoch: 9 Training Loss:  0.620936609407266 Valid Loss:  0.6427202677726745\n",
      "Epoch: 10 Training Loss:  0.6268601865358651 Valid Loss:  0.642654475569725\n",
      "Epoch: 11 Training Loss:  0.6274855069428061 Valid Loss:  0.6424791398644447\n",
      "Epoch: 12 Training Loss:  0.6211905394196511 Valid Loss:  0.6426678097248077\n",
      "Epoch: 13 Training Loss:  0.6227892467578252 Valid Loss:  0.6427657973766326\n",
      "Epoch: 14 Training Loss:  0.620989996790886 Valid Loss:  0.6428040778636932\n",
      "Epoch: 15 Training Loss:  0.6242488385339579 Valid Loss:  0.6427217258016268\n",
      "Epoch: 16 Training Loss:  0.6229659824172655 Valid Loss:  0.6428936017553012\n",
      "Epoch: 17 Training Loss:  0.6209434172709782 Valid Loss:  0.642879164814949\n",
      "Epoch: 18 Training Loss:  0.6232432089249293 Valid Loss:  0.6428864260514577\n",
      "Epoch: 19 Training Loss:  0.620936692516009 Valid Loss:  0.6428706842660904\n",
      "Epoch: 20 Training Loss:  0.6261820699175199 Valid Loss:  0.6427700300017992\n",
      "Epoch: 21 Training Loss:  0.624682057723403 Valid Loss:  0.6426855656504631\n",
      "Epoch: 22 Training Loss:  0.6211937675476075 Valid Loss:  0.642773722410202\n",
      "Epoch: 23 Training Loss:  0.6242740847667059 Valid Loss:  0.6425849271814028\n",
      "Epoch: 24 Training Loss:  0.6222359661459923 Valid Loss:  0.6427015216151873\n",
      "Epoch: 25 Training Loss:  0.6245483973821004 Valid Loss:  0.6427721658349037\n",
      "Epoch: 26 Training Loss:  0.6261564802676439 Valid Loss:  0.6425551855564118\n",
      "Epoch: 27 Training Loss:  0.6232992161015669 Valid Loss:  0.6426793503761291\n",
      "Epoch: 28 Training Loss:  0.6209484625856082 Valid Loss:  0.6427626746892929\n",
      "Epoch: 29 Training Loss:  0.632652715216187 Valid Loss:  0.6423465649286906\n",
      "Epoch: 30 Training Loss:  0.6209446368614833 Valid Loss:  0.6427151477336883\n",
      "Epoch: 31 Training Loss:  0.6215841142733892 Valid Loss:  0.6428508448600769\n",
      "Epoch: 32 Training Loss:  0.6251431435917815 Valid Loss:  0.6427049102385839\n",
      "Epoch: 33 Training Loss:  0.6210840291976929 Valid Loss:  0.6427835631370544\n",
      "Epoch: 34 Training Loss:  0.6220448923309644 Valid Loss:  0.6428094965219497\n",
      "Epoch: 35 Training Loss:  0.620931037346522 Valid Loss:  0.6428282177448272\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-05adc73b9e04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m## 4. weight optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100): ## run the model for 10 epochs\n",
    "    train_loss, valid_loss = [], []\n",
    "    ## training part \n",
    "    model.train()\n",
    "    for data, target in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    ## evaluation part \n",
    "    model.eval()\n",
    "    for data, target in val_dataloader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss.append(loss.item())\n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praty\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, './resnet_transfer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praty\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  0.6592349473051727 Valid Loss:  0.6193452485402425\n",
      "Epoch: 2 Training Loss:  0.6444176991457741 Valid Loss:  0.6194870723287265\n",
      "Epoch: 3 Training Loss:  0.6427005646427473 Valid Loss:  0.6226571521162987\n",
      "Epoch: 4 Training Loss:  0.6497400937958931 Valid Loss:  0.6277181524038314\n",
      "Epoch: 5 Training Loss:  0.6336629179219405 Valid Loss:  0.6203315488497416\n",
      "Epoch: 6 Training Loss:  0.6298922127783299 Valid Loss:  0.6169438596566518\n",
      "Epoch: 7 Training Loss:  0.6285235941310724 Valid Loss:  0.6155502512057622\n",
      "Epoch: 8 Training Loss:  0.6281950875024 Valid Loss:  0.6150892426570257\n",
      "Epoch: 9 Training Loss:  0.6262563546895981 Valid Loss:  0.6147003951668739\n",
      "Epoch: 10 Training Loss:  0.635164418688044 Valid Loss:  0.6144947161277136\n",
      "Epoch: 11 Training Loss:  0.6263223273952802 Valid Loss:  0.6145180338621139\n",
      "Epoch: 12 Training Loss:  0.6280725013166666 Valid Loss:  0.6144808762272199\n",
      "Epoch: 13 Training Loss:  0.6310700549309453 Valid Loss:  0.6146284881234169\n",
      "Epoch: 14 Training Loss:  0.6263651157220205 Valid Loss:  0.6145271003246308\n",
      "Epoch: 15 Training Loss:  0.6261070502400399 Valid Loss:  0.6144773478309313\n",
      "Epoch: 16 Training Loss:  0.6260910466710726 Valid Loss:  0.6144529606898625\n",
      "Epoch: 17 Training Loss:  0.7217695909528811 Valid Loss:  0.6146006261308988\n",
      "Epoch: 18 Training Loss:  0.6274569982141256 Valid Loss:  0.6147240482767423\n",
      "Epoch: 19 Training Loss:  0.6289628180355454 Valid Loss:  0.614684082766374\n",
      "Epoch: 20 Training Loss:  0.6264931009809176 Valid Loss:  0.6145715762178103\n",
      "Epoch: 21 Training Loss:  0.6374490643847385 Valid Loss:  0.6146507320801416\n",
      "Epoch: 22 Training Loss:  0.6261512898405392 Valid Loss:  0.6145382390419643\n",
      "Epoch: 23 Training Loss:  0.9700898595804908 Valid Loss:  0.6144975346326828\n",
      "Epoch: 24 Training Loss:  0.6276716322799524 Valid Loss:  0.6145204532146454\n",
      "Epoch: 25 Training Loss:  0.6264351344704628 Valid Loss:  0.6144702278574308\n",
      "Epoch: 26 Training Loss:  0.6295126035660505 Valid Loss:  0.614619433482488\n",
      "Epoch: 27 Training Loss:  0.626342393318812 Valid Loss:  0.6145212074120839\n",
      "Epoch: 28 Training Loss:  0.6260985462268194 Valid Loss:  0.6144747058550517\n",
      "Epoch: 29 Training Loss:  0.7307036441044961 Valid Loss:  0.6147451701760293\n",
      "Epoch: 30 Training Loss:  0.6261774291793505 Valid Loss:  0.6145827623208364\n",
      "Epoch: 31 Training Loss:  0.6318755395114422 Valid Loss:  0.614368233581384\n",
      "Epoch: 32 Training Loss:  0.627629634598891 Valid Loss:  0.6145796358585358\n",
      "Epoch: 33 Training Loss:  0.6280378702332576 Valid Loss:  0.6146253238121668\n",
      "Epoch: 34 Training Loss:  0.6267601821223895 Valid Loss:  0.614544329047203\n",
      "Epoch: 35 Training Loss:  0.6268050884604454 Valid Loss:  0.6145088679591815\n",
      "Epoch: 36 Training Loss:  0.6282229262689749 Valid Loss:  0.614522955417633\n",
      "Epoch: 37 Training Loss:  0.6274257309834163 Valid Loss:  0.614528146882852\n",
      "Epoch: 38 Training Loss:  0.6301401194160183 Valid Loss:  0.6144837191700936\n",
      "Epoch: 39 Training Loss:  0.6264941944877307 Valid Loss:  0.6144774182637532\n",
      "Epoch: 40 Training Loss:  0.6260987320939699 Valid Loss:  0.614441237548987\n",
      "Epoch: 41 Training Loss:  0.6278406327366829 Valid Loss:  0.6144205154975255\n",
      "Epoch: 42 Training Loss:  0.631754382789135 Valid Loss:  0.6143436860044797\n",
      "Epoch: 43 Training Loss:  0.6280668054521084 Valid Loss:  0.6144550713896751\n",
      "Epoch: 44 Training Loss:  0.6267182628115018 Valid Loss:  0.6144452460606893\n",
      "Epoch: 45 Training Loss:  0.6265310746630033 Valid Loss:  0.614488148589929\n",
      "Epoch: 46 Training Loss:  0.6280039072136084 Valid Loss:  0.61448602805535\n",
      "Epoch: 47 Training Loss:  0.6275037126243115 Valid Loss:  0.614527098039786\n",
      "Epoch: 48 Training Loss:  0.6261195733149847 Valid Loss:  0.6144856191674868\n",
      "Epoch: 49 Training Loss:  0.6260968277851741 Valid Loss:  0.614458348651727\n",
      "Epoch: 50 Training Loss:  0.8775002237060888 Valid Loss:  0.6144857728481292\n",
      "Epoch: 51 Training Loss:  0.6306297389467558 Valid Loss:  0.6147775104641915\n",
      "Epoch: 52 Training Loss:  0.6352654378079048 Valid Loss:  0.6147101726134618\n",
      "Epoch: 53 Training Loss:  0.6268527469436328 Valid Loss:  0.6146217629313468\n",
      "Epoch: 54 Training Loss:  0.630939571728309 Valid Loss:  0.6147154892484347\n",
      "Epoch: 55 Training Loss:  0.6282801622003317 Valid Loss:  0.6145366443196932\n",
      "Epoch: 56 Training Loss:  0.6350024249219957 Valid Loss:  0.6146384542187054\n",
      "Epoch: 57 Training Loss:  0.628974498351415 Valid Loss:  0.6147982963919639\n",
      "Epoch: 58 Training Loss:  0.6269639537731806 Valid Loss:  0.6147724469502767\n",
      "Epoch: 59 Training Loss:  0.626309892932574 Valid Loss:  0.6145853505531946\n",
      "Epoch: 60 Training Loss:  0.6261798231005669 Valid Loss:  0.6145015543699265\n",
      "Epoch: 61 Training Loss:  0.626501060406367 Valid Loss:  0.6144763217369715\n",
      "Epoch: 62 Training Loss:  1.1148767982095615 Valid Loss:  0.6148404451211293\n",
      "Epoch: 63 Training Loss:  0.6263551546136538 Valid Loss:  0.6146579996744792\n",
      "Epoch: 64 Training Loss:  0.6267458196679752 Valid Loss:  0.6145755269130071\n",
      "Epoch: 65 Training Loss:  0.6262338589628538 Valid Loss:  0.6144967927535375\n",
      "Epoch: 66 Training Loss:  0.8597482471064352 Valid Loss:  0.614412323931853\n",
      "Epoch: 67 Training Loss:  0.630549006069079 Valid Loss:  0.6146595754226049\n",
      "Epoch: 68 Training Loss:  0.6261398184696834 Valid Loss:  0.6145458395282427\n",
      "Epoch: 69 Training Loss:  0.628697696035107 Valid Loss:  0.6145680033167203\n",
      "Epoch: 70 Training Loss:  0.6314646801877146 Valid Loss:  0.6146734997630119\n",
      "Epoch: 71 Training Loss:  0.626546686510245 Valid Loss:  0.6146126291155816\n",
      "Epoch: 72 Training Loss:  0.6266714991132418 Valid Loss:  0.6145049491524697\n",
      "Epoch: 73 Training Loss:  0.8385094172937751 Valid Loss:  0.6145655410488446\n",
      "Epoch: 74 Training Loss:  0.6275494316021601 Valid Loss:  0.6145067662994067\n",
      "Epoch: 75 Training Loss:  0.6261692696213722 Valid Loss:  0.6144904061158498\n",
      "Epoch: 76 Training Loss:  0.630742215782404 Valid Loss:  0.6145509879787763\n",
      "Epoch: 77 Training Loss:  0.6288896472950777 Valid Loss:  0.6146435914436976\n",
      "Epoch: 78 Training Loss:  0.631026876210856 Valid Loss:  0.6147975577910741\n",
      "Epoch: 79 Training Loss:  0.6280830468386411 Valid Loss:  0.6146644008159637\n",
      "Epoch: 80 Training Loss:  0.6268463649153709 Valid Loss:  0.6146748073895772\n",
      "Epoch: 81 Training Loss:  0.6272522400816282 Valid Loss:  0.6145023433367411\n",
      "Epoch: 82 Training Loss:  0.6284065425743659 Valid Loss:  0.6145733910799026\n",
      "Epoch: 83 Training Loss:  0.6351199842319281 Valid Loss:  0.6146971381704013\n",
      "Epoch: 84 Training Loss:  0.6263004978696505 Valid Loss:  0.6145463566978773\n",
      "Epoch: 85 Training Loss:  0.6264681080977123 Valid Loss:  0.6145044333736102\n",
      "Epoch: 86 Training Loss:  0.8372967421317444 Valid Loss:  0.6146732770403226\n",
      "Epoch: 87 Training Loss:  0.6262361393173536 Valid Loss:  0.6145426797866821\n",
      "Epoch: 88 Training Loss:  0.626108703494072 Valid Loss:  0.614488999247551\n",
      "Epoch: 89 Training Loss:  0.6299578510522842 Valid Loss:  0.6144435252745947\n",
      "Epoch: 90 Training Loss:  0.6352883479333735 Valid Loss:  0.6146892458200455\n",
      "Epoch: 91 Training Loss:  0.6281339719494183 Valid Loss:  0.6146656027436257\n",
      "Epoch: 92 Training Loss:  0.6262798801461855 Valid Loss:  0.6145445445179939\n",
      "Epoch: 93 Training Loss:  0.6262994085351626 Valid Loss:  0.6144730669260025\n",
      "Epoch: 94 Training Loss:  0.6282807552864155 Valid Loss:  0.6145212016503017\n",
      "Epoch: 95 Training Loss:  1.0583779275486014 Valid Loss:  0.6148017012079556\n",
      "Epoch: 96 Training Loss:  0.6274727782482902 Valid Loss:  0.614720813135306\n",
      "Epoch: 97 Training Loss:  0.6261440376838048 Valid Loss:  0.6145644857486089\n",
      "Epoch: 98 Training Loss:  0.6317682918856541 Valid Loss:  0.6145401581128438\n",
      "Epoch: 99 Training Loss:  0.6267781744599342 Valid Loss:  0.614603026509285\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100): ## run the model for 10 epochs\n",
    "    train_loss, valid_loss = [], []\n",
    "    ## training part \n",
    "    model.train()\n",
    "    for data, target in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    ## evaluation part \n",
    "    model.eval()\n",
    "    for data, target in val_dataloader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss.append(loss.item())\n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
